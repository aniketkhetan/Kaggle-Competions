{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,cross_val_predict,GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Evaluation function\n",
    "def Report(model):\n",
    "    print(model.best_params_)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print('Accuracy Score:',metrics.accuracy_score(y_test,y_pred))\n",
    "    print('Precision Score:',metrics.precision_score(y_test,y_pred))\n",
    "    print('Recall Score:',metrics.recall_score(y_test,y_pred))\n",
    "    print('F1 Score:',metrics.f1_score(y_test,y_pred))\n",
    "    print('Confusion Matrix:',metrics.confusion_matrix(y_test,y_pred))\n",
    "    print('Classification Report:',metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GridSearch Function\n",
    "\n",
    "def train_model(model,parameters={}):\n",
    "    grid = GridSearchCV(model,param_grid=  parameters,cv=5,refit=True)\n",
    "    grid.fit(x_train,y_train)\n",
    "    return grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Cabin          0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handling Training Missing Values\n",
    "df.isna().sum()\n",
    "df['Age'].fillna(df['Age'].mean(), axis=0,inplace=True)\n",
    "# df['Cabin'].fillna(df['Cabin'].value_counts().idxmax(),axis=0 ,inplace=True)\n",
    "df.dropna(subset='Cabin',how='any',axis=0, inplace=True)\n",
    "df.dropna(subset='Embarked',how='any',axis=0, inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Cabin          0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handling Test Missing Values\n",
    "df_test.isna().sum()\n",
    "df_test['Age'].fillna(df_test['Age'].mean(), axis=0,inplace=True)\n",
    "df_test['Cabin'].fillna(df_test['Cabin'].value_counts().idxmax(),axis=0 ,inplace=True)\n",
    "df_test['Fare'].fillna(df_test['Fare'].mean(), axis=0,inplace=True)\n",
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorical variable into indicator variables\n",
    "label_encoder = LabelEncoder()\n",
    "#train data \n",
    "df['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
    "df['Embarked'] = label_encoder.fit_transform(df['Embarked'])\n",
    "df['Cabin'] = label_encoder.fit_transform(df['Cabin'])\n",
    "df['Ticket'] = label_encoder.fit_transform(df['Ticket'])\n",
    "\n",
    "#test data \n",
    "df_test['Sex'] = label_encoder.fit_transform(df_test['Sex'])\n",
    "df_test['Embarked'] = label_encoder.fit_transform(df_test['Embarked'])\n",
    "df_test['Cabin'] = label_encoder.fit_transform(df_test['Cabin']) \n",
    "df_test['Ticket'] = label_encoder.fit_transform(df_test['Ticket'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data \n",
    "\n",
    "train_label = df['Survived']\n",
    "train_data = df.drop(['Survived','Name','PassengerId'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#test data\n",
    "\n",
    "test_data = df_test.drop(['Name','PassengerId'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Model and predicting labels for test data\n",
    "lr = LogisticRegression(penalty='l2',solver='newton-cg',max_iter=1000)\n",
    "lr.fit(train_data,train_label)\n",
    "pred_label = lr.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting predictions in submission format\n",
    "pred_label= pd.DataFrame(pred_label,index = df_test['PassengerId'])\n",
    "pred_label.rename(columns={0:'Survived'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model by generating test split from train.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(train_data,train_label,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'newton-cg'}\n",
      "Accuracy Score: 0.6585365853658537\n",
      "Precision Score: 0.6666666666666666\n",
      "Recall Score: 0.88\n",
      "F1 Score: 0.7586206896551725\n",
      "Confusion Matrix: [[ 5 11]\n",
      " [ 3 22]]\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.31      0.42        16\n",
      "           1       0.67      0.88      0.76        25\n",
      "\n",
      "    accuracy                           0.66        41\n",
      "   macro avg       0.65      0.60      0.59        41\n",
      "weighted avg       0.65      0.66      0.63        41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/test/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/test/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/test/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/test/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/test/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/test/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/test/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/test/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/test/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/test/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lre = train_model(LogisticRegression(penalty='l2',max_iter=1000),{'solver':['newton-cg','lbfgs','liblinear','sag','saga']})\n",
    "\n",
    "Report(lre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEnter the number of rows:\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'A'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124751669\n",
      "3537170686552920\n",
      "27151730669\n",
      "125731446446669\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_itr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(t):\n\u001b[1;32m      5\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m()\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m----> 7\u001b[0m     total \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m  i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(total))\n",
      "Cell \u001b[0;32mIn[79], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_itr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(t):\n\u001b[1;32m      5\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m()\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m----> 7\u001b[0m     total \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m  \u001b[43mi\u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(total))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = int(input().strip())\n",
    "    \n",
    "for t_itr in range(t):\n",
    "        \n",
    "    n = int(input().strip())\n",
    "    \n",
    "    total = [i for i in range(n) if i%3==0 or  i%5==0]\n",
    "    print(sum(total))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
